# README2: Cart Events API & Transformation Pipeline - Flow & Logic

> Chi tiáº¿t vá» luá»“ng xá»­ lÃ½, logic vÃ  thá»© tá»± cháº¡y cá»§a Cart Events API vÃ  Transformation Pipeline

---

## ğŸ“‹ Má»¤C Lá»¤C

1. [Tá»•ng Quan Kiáº¿n TrÃºc](#tá»•ng-quan-kiáº¿n-trÃºc)
2. [API Events - Luá»“ng & Logic](#api-events---luá»“ng--logic)
3. [Transformation Pipeline - Logic](#transformation-pipeline---logic)
4. [Docker Integration Flow](#docker-integration-flow)
5. [Luá»“ng HoÃ n Chá»‰nh End-to-End](#luá»“ng-hoÃ n-chá»‰nh-end-to-end)

---

## ğŸ—ï¸ Tá»”NG QUAN KIáº¾N TRÃšC

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        CART EVENTS SYSTEM                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

PHASE 1: DATA GENERATION (API)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   FastAPI Server         â”‚
â”‚   routers/               â”‚
â”‚   cart_tracking_router.pyâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â”œâ”€â†’ Generate Events (POST /cart/generate/events)
           â”‚   â””â”€â†’ Save: shared_data/private_data/cart_tracking/cart_events.json.gz
           â”‚
           â”œâ”€â†’ Query Events (GET /cart/events)
           â”œâ”€â†’ Filter by Customer/Product/Session
           â”œâ”€â†’ Statistics (GET /cart/statistics)
           â””â”€â†’ Abandoned Carts (GET /cart/abandoned)

           â†“

PHASE 2: TRANSFORMATION (Python Scripts)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   transformation/        â”‚
â”‚   â”œâ”€ transform_*.py      â”‚
â”‚   â””â”€ aggregate_*.py      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â”œâ”€â†’ Load: cart_events.json.gz
           â”œâ”€â†’ Clean & Deduplicate
           â”œâ”€â†’ Create User Journeys
           â”œâ”€â†’ Save: Partitioned Parquet Files
           â””â”€â†’ Aggregate: Session Metrics

           â†“

PHASE 3: DOCKER ORCHESTRATION (Optional)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Docker Containers      â”‚
â”‚   docker/                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â”œâ”€â†’ Single Worker: Process entire file
           â”‚
           â””â”€â†’ Worker Pool: Parallel processing
               â”œâ”€ Redis Queue
               â”œâ”€ Scheduler splits work
               â””â”€ 4-8 Workers process chunks
```

---

## ğŸ“¡ API EVENTS - LUá»’NG & LOGIC

### **NhÃ³m API Endpoints**

#### **1. Information Endpoint**
```
GET /cart/
```
**Má»¥c Ä‘Ã­ch:** Xem thÃ´ng tin tá»•ng quan API
**Response:** Danh sÃ¡ch endpoints vÃ  sá»‘ lÆ°á»£ng events hiá»‡n cÃ³

---

#### **2. Query Endpoints** (Äá»c dá»¯ liá»‡u)

##### **2.1. Get All Events**
```
GET /cart/events?limit=100&offset=0&event_type=add_to_cart
```

**Logic:**
1. Load file `cart_events.json.gz`
2. Apply filters (náº¿u cÃ³):
   - `event_type`: add_to_cart, remove_from_cart, update_quantity, etc.
   - `source`: website, mobile_app, mobile_web
   - `device`: desktop, mobile, tablet
   - `start_date`, `end_date`: Filter theo thá»i gian
3. Sort by timestamp (descending)
4. Pagination: offset â†’ offset + limit
5. Return: events + count + total

**Use case:** Láº¥y danh sÃ¡ch events vá»›i filter

---

##### **2.2. Get Events by Customer**
```
GET /cart/events/customer/{customer_id}?limit=100
```

**Logic:**
1. Load events
2. Filter: `customer_id == {customer_id}`
3. Sort by timestamp desc
4. Limit results
5. Return customer's journey

**Use case:** Xem lá»‹ch sá»­ mua sáº¯m cá»§a 1 khÃ¡ch hÃ ng

---

##### **2.3. Get Events by Product**
```
GET /cart/events/product/{product_id}?limit=100
```

**Logic:**
1. Load events
2. Filter: `product_id == {product_id}`
3. Sort by timestamp desc
4. Return product interaction history

**Use case:** PhÃ¢n tÃ­ch sáº£n pháº©m nÃ o Ä‘Æ°á»£c add/remove nhiá»u

---

##### **2.4. Get Events by Session**
```
GET /cart/events/session/{session_id}?limit=100
```

**Logic:**
1. Load events
2. Filter: `session_id == {session_id}`
3. Sort by timestamp asc (chronological order)
4. Return complete user journey trong 1 session

**Use case:** Tracking user behavior trong 1 phiÃªn

---

#### **3. Analytics Endpoints**

##### **3.1. Statistics**
```
GET /cart/statistics
```

**Logic:**
1. Load táº¥t cáº£ events
2. Aggregate:
   ```python
   - Count by event_type
   - Count by source (website/mobile/app)
   - Count by device (desktop/mobile/tablet)
   - Count unique customers
   - Count unique sessions
   - Calculate add/remove ratio
   - Top 10 products added to cart
   ```
3. Return summary statistics

**Output:**
```json
{
  "total_events": 10000,
  "by_event_type": {
    "add_to_cart": 3387,
    "update_quantity": 3350,
    "remove_from_cart": 3263
  },
  "unique_customers": 1000,
  "unique_sessions": 1000,
  "add_remove_ratio": 1.04,
  "top_products_added": [...]
}
```

---

##### **3.2. Abandoned Carts**
```
GET /cart/abandoned?limit=50&hours_threshold=24
```

**Logic:**
1. Load events
2. Group by `session_id`
3. For each session:
   ```python
   cart = {}
   for event in session_events:
       if event_type == "add_to_cart":
           cart[product_id].quantity += quantity
       elif event_type == "remove_from_cart":
           cart[product_id].quantity -= quantity
       elif event_type == "update_quantity":
           cart[product_id].quantity = quantity
   ```
4. Filter sessions where:
   - `cart` has items (not empty)
   - `last_activity` > `hours_threshold`
5. Calculate `cart_value_vnd`
6. Sort by cart value descending
7. Return top abandoned carts

**Use case:** Recovery campaigns cho abandoned carts

---

#### **4. Generation Endpoints**

##### **4.1. Generate Events**
```
POST /cart/generate/events?count=10000&method=new
```

**Logic:**
1. **Check existing data:**
   - If exists vÃ  `method != "new"` â†’ Warning (Ä‘á»ƒ trÃ¡nh ghi Ä‘Ã¨ nháº§m)
   - If exists vÃ  `method == "new"` â†’ Append mode
   - If not exists â†’ Replace mode

2. **Start background generation:**
   ```python
   generation_status = {
       "is_generating": True,
       "cart_events": {
           "target": count,
           "generated": 0,
           "completed": False
       }
   }
   ```

3. **Generate in batches:**
   ```python
   batch_size = 50000
   num_batches = count / batch_size

   for batch in num_batches:
       # Generate batch
       events = generate_cart_events_batch(batch_size)

       # Update progress
       progress_percentage = (generated / target) * 100

       # Calculate ETA
       elapsed_time = now - start_time
       avg_time_per_event = elapsed_time / generated
       remaining_time = avg_time_per_event * remaining_events
   ```

4. **Generate logic per event:**
   ```python
   # Create sessions (~1000 sessions for 10K events)
   num_sessions = count // 10

   for session in sessions:
       # 30% guest users (customer_id = null)
       is_guest = random() < 0.3

       # Session context
       session = {
           "session_id": generate_session_id(),
           "customer_id": random_customer() if not is_guest else null,
           "source": random_choice([website, mobile_app, mobile_web]),
           "device": random_choice([desktop, mobile, tablet]),
           "browser": random_choice([Chrome, Safari, Firefox, Edge])
       }

   for i in range(count):
       # Pick random session
       session = random_choice(sessions)

       # Generate event
       event = {
           "event_id": generate_uuid(),
           "event_type": random_choice(event_types),
           "timestamp": now - random_timedelta(0-90 days),
           "session_id": session.session_id,
           "customer_id": session.customer_id,
           ...
       }

       # Add event-specific data
       if event_type == "add_to_cart":
           event.update({
               "product_id": random_product(),
               "quantity": random(1-5),
               "price_vnd": product.price,
               "line_total_vnd": price * quantity
           })
       elif event_type == "remove_from_cart":
           # 30% remove all
           quantity = 0 if random() > 0.7 else random(1-5)
           ...
   ```

5. **Sort and save:**
   ```python
   events.sort(key=lambda x: x["timestamp"])
   save_compressed(events, "cart_events.json.gz")
   ```

6. **Update status:**
   ```python
   generation_status["completed"] = True
   generation_status["is_generating"] = False
   ```

**Parameters:**
- `count`: 100 - 1,000,000 events
- `method=new`: Append to existing data

**Event Types Generated:**
```
- add_to_cart          â†’ Cart modification events
- remove_from_cart
- update_quantity
- view_item            â†’ Browse events
- purchase             â†’ Transaction events
- scroll, exit_page    â†’ Engagement events
- search               â†’ Search events
- add_to_wish_list     â†’ Interest events
- begin_checkout       â†’ Funnel events
- add_shipping_info
- add_payment_info
- payment_failed       â†’ Error events
- order_cancelled
```

---

##### **4.2. Generation Status**
```
GET /cart/generate/status
```

**Logic:**
1. Check `generation_status["is_generating"]`
2. If generating:
   ```python
   return {
       "progress_percentage": 45.2,
       "events_generated": 4520,
       "target_events": 10000,
       "elapsed_time_minutes": 1.5,
       "estimated_time_remaining": "1.8 minutes",
       "events_per_second": 50.22
   }
   ```
3. If completed:
   ```python
   return {
       "generation_status": "completed",
       "total_events": 10000
   }
   ```
4. If idle:
   ```python
   return {
       "generation_status": "idle",
       "instructions": "Call POST /cart/generate/events"
   }
   ```

**Use case:** Monitor generation progress

---

### **Data Flow Diagram - API**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    API DATA FLOW                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

1. GENERATION FLOW
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

User Request
   â”‚
   â”œâ”€â†’ POST /cart/generate/events?count=10000
   â”‚
   â”œâ”€â†’ Check existing data
   â”‚   â”œâ”€ Exists? â†’ Warning (unless method=new)
   â”‚   â””â”€ Not exists? â†’ Proceed
   â”‚
   â”œâ”€â†’ Start background task
   â”‚   â””â”€â†’ generation_status["is_generating"] = True
   â”‚
   â”œâ”€â†’ Generate in batches (50K per batch)
   â”‚   â”‚
   â”‚   â”œâ”€â†’ Create sessions (10% of events)
   â”‚   â”‚   â””â”€â†’ 30% guest users (customer_id = null)
   â”‚   â”‚
   â”‚   â”œâ”€â†’ For each event:
   â”‚   â”‚   â”œâ”€ Pick random session
   â”‚   â”‚   â”œâ”€ Pick random product
   â”‚   â”‚   â”œâ”€ Generate timestamp (0-90 days ago)
   â”‚   â”‚   â”œâ”€ Pick event_type
   â”‚   â”‚   â””â”€ Add event-specific data
   â”‚   â”‚
   â”‚   â”œâ”€â†’ Update progress
   â”‚   â”‚   â”œâ”€ Calculate: progress_percentage
   â”‚   â”‚   â”œâ”€ Calculate: ETA
   â”‚   â”‚   â””â”€ Log progress
   â”‚   â”‚
   â”‚   â””â”€â†’ Sort by timestamp
   â”‚
   â”œâ”€â†’ Save to file
   â”‚   â””â”€â†’ shared_data/private_data/cart_tracking/cart_events.json.gz
   â”‚
   â””â”€â†’ Update status: completed = True

Monitor Progress
   â”‚
   â””â”€â†’ GET /cart/generate/status
       â””â”€â†’ Real-time progress updates


2. QUERY FLOW
â•â•â•â•â•â•â•â•â•â•â•â•â•

User Query
   â”‚
   â”œâ”€â†’ GET /cart/events?filters
   â”‚
   â”œâ”€â†’ Load cart_events.json.gz
   â”‚   â””â”€â†’ Decompress gzip
   â”‚       â””â”€â†’ Parse JSON
   â”‚
   â”œâ”€â†’ Apply filters
   â”‚   â”œâ”€ event_type
   â”‚   â”œâ”€ source
   â”‚   â”œâ”€ device
   â”‚   â”œâ”€ date range
   â”‚   â””â”€ customer/product/session
   â”‚
   â”œâ”€â†’ Sort by timestamp
   â”‚
   â”œâ”€â†’ Pagination (offset, limit)
   â”‚
   â””â”€â†’ Return {data, count, total}


3. ANALYTICS FLOW
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Statistics Request
   â”‚
   â”œâ”€â†’ GET /cart/statistics
   â”‚
   â”œâ”€â†’ Load all events
   â”‚
   â”œâ”€â†’ Aggregate
   â”‚   â”œâ”€ Count by event_type
   â”‚   â”œâ”€ Count by source
   â”‚   â”œâ”€ Count by device
   â”‚   â”œâ”€ Count unique customers
   â”‚   â”œâ”€ Count unique sessions
   â”‚   â”œâ”€ Calculate add/remove ratio
   â”‚   â””â”€ Top 10 products
   â”‚
   â””â”€â†’ Return statistics JSON

Abandoned Carts Request
   â”‚
   â”œâ”€â†’ GET /cart/abandoned?hours_threshold=24
   â”‚
   â”œâ”€â†’ Load all events
   â”‚
   â”œâ”€â†’ Group by session_id
   â”‚
   â”œâ”€â†’ For each session:
   â”‚   â”œâ”€ Calculate current cart state
   â”‚   â”‚   â”œâ”€ Process add_to_cart events
   â”‚   â”‚   â”œâ”€ Process remove_from_cart events
   â”‚   â”‚   â””â”€ Process update_quantity events
   â”‚   â”‚
   â”‚   â”œâ”€ Check last_activity
   â”‚   â”‚   â””â”€ If > hours_threshold â†’ Abandoned
   â”‚   â”‚
   â”‚   â””â”€ Calculate cart_value_vnd
   â”‚
   â”œâ”€â†’ Sort by cart_value descending
   â”‚
   â””â”€â†’ Return top abandoned carts
```

---

## ğŸ”„ TRANSFORMATION PIPELINE - LOGIC

### **Thá»© Tá»± Cháº¡y Transformation**

```
INPUT: cart_events.json.gz (10K-2M+ events)
   â”‚
   â”œâ”€â†’ STEP 1: Load Data
   â”œâ”€â†’ STEP 2: Clean Data
   â”œâ”€â†’ STEP 3: Deduplicate
   â”œâ”€â†’ STEP 4: Create User Journeys
   â”œâ”€â†’ STEP 5: Save Parquet (Partitioned)
   â”œâ”€â†’ STEP 6: Generate Session Metrics
   â””â”€â†’ STEP 7: Aggregate Metrics

OUTPUT:
   â”œâ”€ cart_events_cleaned/ (Parquet, partitioned by date)
   â”œâ”€ session_metrics.parquet
   â””â”€ aggregation_report.txt
```

---

### **STEP 1: Load Data**

**File:** `transform_cart_events.py`, `transform_cart_events_extreme.py`

**Standard Version:**
```python
def load_data():
    with gzip.open('cart_events.json.gz', 'rt') as f:
        data = json.load(f)  # Load toÃ n bá»™ vÃ o memory
    df = pd.DataFrame(data)
    return df

# Memory: ~3-5x data size
# 10K events: ~30MB
# 2M events: ~6-12GB âŒ Out of Memory
```

**Extreme Version (Streaming):**
```python
def load_events_streaming():
    import ijson  # Streaming JSON parser

    with gzip.open('cart_events.json.gz', 'rb') as f:
        # Parse tá»«ng event má»™t, khÃ´ng load háº¿t vÃ o memory
        parser = ijson.items(f, 'item')
        for event in parser:
            yield event  # Generator, chá»‰ giá»¯ 1 event táº¡i 1 thá»i Ä‘iá»ƒm

# Memory: Constant ~500MB
# 10K events: ~300MB
# 2M events: ~500MB âœ… Success
```

**Logic:**
1. Má»Ÿ file gzip
2. Parse JSON array
3. Extreme version: Stream tá»«ng item
4. Standard version: Load toÃ n bá»™

---

### **STEP 2: Clean Data**

```python
def clean_data(df):
    # 1. Convert timestamp
    df['timestamp'] = pd.to_datetime(df['timestamp'])
    df['date'] = df['timestamp'].dt.date
    df['hour'] = df['timestamp'].dt.hour

    # 2. Remove missing critical fields
    critical = ['event_id', 'session_id', 'customer_id', 'event_type']
    df = df.dropna(subset=critical)

    # 3. Handle missing UTM fields
    utm_fields = ['utm_source', 'utm_medium', 'utm_campaign']
    for field in utm_fields:
        df[field] = df[field].fillna('unknown')

    # 4. Handle missing referrer
    df['referrer'] = df['referrer'].fillna('direct')

    # 5. Ensure numeric types
    numeric_fields = ['product_price_vnd', 'quantity', 'line_total_vnd']
    for field in numeric_fields:
        df[field] = pd.to_numeric(df[field], errors='coerce').fillna(0)

    # 6. Remove invalid records
    df = df[df['product_price_vnd'] >= 0]
    df = df[df['quantity'] >= 0]

    return df
```

**Output:** Clean DataFrame

---

### **STEP 3: Deduplicate**

**Standard Version:**
```python
def deduplicate(df):
    initial_count = len(df)

    # Remove duplicates based on event_id
    df = df.drop_duplicates(subset=['event_id'], keep='first')

    duplicates_removed = initial_count - len(df)
    return df
```

**Extreme Version (Incremental):**
```python
seen_event_ids = set()  # Track across batches

def deduplicate_batch(df, seen_ids):
    # Filter already seen
    mask = ~df['event_id'].isin(seen_ids)
    df = df[mask]

    # Remove duplicates within batch
    df = df.drop_duplicates(subset=['event_id'], keep='first')

    # Update seen set
    seen_ids.update(df['event_id'].tolist())

    return df, seen_ids
```

**Logic:**
1. Track `event_id` Ä‘Ã£ tháº¥y
2. Remove events vá»›i `event_id` trÃ¹ng
3. Keep first occurrence

---

### **STEP 4: Create User Journeys**

```python
def create_user_journeys(df):
    # 1. Sort by session and timestamp
    df = df.sort_values(['session_id', 'timestamp'])

    # 2. Add sequence number within session
    df['event_sequence_num'] = df.groupby('session_id').cumcount() + 1

    # 3. Calculate session-level metrics
    session_metrics = df.groupby('session_id').agg({
        'timestamp': ['min', 'max', 'count'],
        'event_type': lambda x: ','.join(x),
        'customer_id': 'first',
        'source': 'first',
        'device': 'first'
    })

    # 4. Flatten columns
    session_metrics.columns = [
        'session_id', 'session_start', 'session_end',
        'total_events', 'event_journey', 'customer_id',
        'source', 'device'
    ]

    # 5. Calculate session duration
    session_metrics['session_duration_seconds'] = (
        session_metrics['session_end'] - session_metrics['session_start']
    ).dt.total_seconds()

    # 6. Merge back to main dataframe
    df = df.merge(session_metrics, on='session_id', how='left')

    return df, session_metrics
```

**Output:**
```python
# Each event cÃ³ thÃªm:
df['event_sequence_num']        # 1, 2, 3, ...
df['session_start']             # First event timestamp
df['session_end']               # Last event timestamp
df['session_duration_seconds']  # Duration
df['total_events']              # Number of events in session
df['event_journey']             # "add_to_cart,update_quantity,remove_from_cart"
```

**Example Journey:**
```json
{
  "session_id": "sess_abc123",
  "customer_id": 445412,
  "event_journey": "add_to_cart â†’ update_quantity â†’ add_to_cart â†’ remove_from_cart",
  "total_events": 4,
  "session_duration_seconds": 245,
  "events": [
    {"event_sequence_num": 1, "event_type": "add_to_cart", "product_id": 90},
    {"event_sequence_num": 2, "event_type": "update_quantity", "product_id": 90},
    {"event_sequence_num": 3, "event_type": "add_to_cart", "product_id": 73},
    {"event_sequence_num": 4, "event_type": "remove_from_cart", "product_id": 90}
  ]
}
```

---

### **STEP 5: Save Parquet (Partitioned)**

```python
def save_to_parquet(df, partition_by='date'):
    output_path = "output/cart_events_cleaned"

    df.to_parquet(
        output_path,
        engine='pyarrow',
        partition_cols=['date'],      # Partition by date
        compression='snappy',          # Fast compression
        index=False
    )
```

**Output Structure:**
```
output/cart_events_cleaned/
â”œâ”€â”€ date=2025-08-25/
â”‚   â””â”€â”€ xxx.parquet
â”œâ”€â”€ date=2025-08-26/
â”‚   â””â”€â”€ xxx.parquet
â”œâ”€â”€ date=2025-08-27/
â”‚   â””â”€â”€ xxx.parquet
...
â””â”€â”€ date=2025-11-24/
    â””â”€â”€ xxx.parquet

Total: 92 date partitions (3 months of data)
```

**Benefits:**
- Query by date: `SELECT * WHERE date='2025-08-25'` â†’ Chá»‰ Ä‘á»c 1 partition
- Columnar storage: Äá»c nhanh, compression tá»‘t
- Snappy: Fast compression (~2x smaller)

---

### **STEP 6: Generate Session Metrics**

```python
def generate_session_metrics():
    session_list = []

    for session_id, data in session_data.items():
        timestamps = data['timestamps']

        session_list.append({
            'session_id': session_id,
            'customer_id': data['customer_id'],
            'source': data['source'],
            'device': data['device'],
            'session_start': min(timestamps),
            'session_end': max(timestamps),
            'total_events': len(data['events']),
            'event_journey': ','.join(data['events']),
            'session_duration_seconds': (max - min).total_seconds(),
            'has_purchase': False  # Placeholder
        })

    df = pd.DataFrame(session_list)
    df.to_parquet('session_metrics.parquet', index=False)

    return df
```

**Output:** `session_metrics.parquet`
- 1 row per session
- Summary cá»§a táº¥t cáº£ events trong session
- Use for session-level analysis

---

### **STEP 7: Aggregate Metrics**

**File:** `aggregate_metrics.py`

```python
def aggregate_all_metrics():
    # Load transformed data
    events_df = pd.read_parquet('cart_events_cleaned/')
    session_df = pd.read_parquet('session_metrics.parquet')

    # 1. Session Duration Statistics
    session_stats = {
        'average_duration_seconds': session_df['session_duration_seconds'].mean(),
        'median_duration_seconds': session_df['session_duration_seconds'].median(),
        'min_duration': session_df['session_duration_seconds'].min(),
        'max_duration': session_df['session_duration_seconds'].max()
    }

    # 2. Purchase Intent Analysis
    last_events = events_df.groupby('session_id').last()
    purchase_intent = (
        (last_events['event_type'] == 'add_to_cart') &
        (events_df.groupby('session_id').size() >= 2)
    )
    purchase_stats = {
        'sessions_with_purchase_intent': purchase_intent.sum(),
        'purchase_intent_rate': purchase_intent.mean() * 100
    }

    # 3. Event Statistics
    event_stats = {
        'total_events': len(events_df),
        'unique_sessions': events_df['session_id'].nunique(),
        'unique_customers': events_df['customer_id'].nunique(),
        'unique_products': events_df['product_id'].nunique(),
        'event_type_distribution': events_df['event_type'].value_counts().to_dict(),
        'source_distribution': events_df['source'].value_counts().to_dict(),
        'device_distribution': events_df['device'].value_counts().to_dict()
    }

    # 4. Journey Patterns
    journey_patterns = events_df.groupby('session_id')['event_type'].apply(
        lambda x: ' -> '.join(x)
    ).value_counts().head(20)

    # 5. Time-based Metrics
    time_metrics = {
        'events_by_hour': events_df.groupby('hour').size().to_dict(),
        'events_by_date': events_df.groupby('date').size().to_dict()
    }

    # Save all metrics
    save_metrics({
        'session_duration': session_stats,
        'purchase_intent': purchase_stats,
        'events': event_stats,
        'journeys': journey_patterns,
        'time': time_metrics
    })
```

**Output:**
```
aggregation_report.txt:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
SESSION DURATION STATISTICS
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Total Sessions: 1,000
Average Duration: 105,220.91 minutes
Median Duration: 109,144.69 minutes

PURCHASE INTENT STATISTICS
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Sessions with Purchase Intent: 330
Purchase Intent Rate: 33.00%

EVENT STATISTICS
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Total Events: 10,000
Unique Sessions: 1,000
Unique Customers: 1,000
Event Type Distribution:
  - add_to_cart: 3,387
  - update_quantity: 3,350
  - remove_from_cart: 3,263

TOP JOURNEY PATTERNS
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
1. update_quantity â†’ remove_from_cart â†’ update_quantity: 2 sessions
2. update_quantity â†’ add_to_cart â†’ add_to_cart: 2 sessions
...
```

---

## ğŸ³ DOCKER INTEGRATION FLOW

### **Architecture Options**

#### **Option 1: Single Container**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Docker Container                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚  Python Environment                â”‚     â”‚
â”‚  â”‚  â”œâ”€ pandas, pyarrow, ijson        â”‚     â”‚
â”‚  â”‚  â””â”€ transformation scripts         â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚                                              â”‚
â”‚  Volume Mounts:                              â”‚
â”‚  /data/input  â†’ ./data/input                â”‚
â”‚  /data/output â†’ ./data/output               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

COMMAND:
docker-compose up

PROCESSING FLOW:
1. Container starts
2. Loads ../shared_data/private_data/cart_tracking/cart_events.json.gz
3. Runs transform_cart_events_extreme.py
4. Saves output to /data/output
5. Container stops
```

---

#### **Option 2: Worker Pool** (Recommended)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      WORKER POOL ARCHITECTURE                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   Redis     â”‚ â† Task Queue
                    â”‚  Container  â”‚
                    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                  â”‚                  â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Scheduler    â”‚ â”‚   Worker 1     â”‚ â”‚   Worker N     â”‚
â”‚   Container    â”‚ â”‚   Container    â”‚ â”‚   Container    â”‚
â”‚                â”‚ â”‚                â”‚ â”‚                â”‚
â”‚ scheduler.py   â”‚ â”‚  worker.py     â”‚ â”‚  worker.py     â”‚
â”‚  â”œâ”€ Split work â”‚ â”‚  â”œâ”€ Process    â”‚ â”‚  â”œâ”€ Process    â”‚
â”‚  â”œâ”€ Enqueue    â”‚ â”‚  â”‚   chunk 1   â”‚ â”‚  â”‚   chunk N   â”‚
â”‚  â””â”€ Monitor    â”‚ â”‚  â””â”€ Save       â”‚ â”‚  â””â”€ Save       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚                  â”‚                  â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
                    â”‚   Storage   â”‚
                    â”‚  /data/     â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**FLOW:**

**Step 1: Start Services**
```bash
docker-compose -f docker-compose.workers.yml up --scale worker=4
```

**Step 2: Services Start**
```
1. Redis container starts
   â””â”€â†’ Task queue ready

2. Scheduler container starts
   â”œâ”€â†’ Connects to Redis
   â”œâ”€â†’ Analyzes input file
   â”œâ”€â†’ Splits work into chunks
   â””â”€â†’ Enqueues jobs to Redis

3. Worker containers start (x4)
   â”œâ”€â†’ Connect to Redis
   â””â”€â†’ Listen for jobs
```

**Step 3: Job Distribution**
```python
# Scheduler (scheduler.py)
def split_work(file, num_workers=4):
    total_events = count_events(file)  # 2M events
    chunk_size = total_events // num_workers  # 500K each

    chunks = [
        {"start": 0, "end": 500000, "chunk_num": 0},
        {"start": 500000, "end": 1000000, "chunk_num": 1},
        {"start": 1000000, "end": 1500000, "chunk_num": 2},
        {"start": 1500000, "end": 2000000, "chunk_num": 3}
    ]

    # Enqueue to Redis
    for chunk in chunks:
        queue.enqueue(process_chunk, chunk)
```

**Step 4: Worker Processing**
```python
# Worker (worker.py)
def process_chunk(chunk_info):
    # Load chunk
    events = load_events(
        file=chunk_info['input_file'],
        start=chunk_info['start'],
        end=chunk_info['end']
    )

    # Transform
    transformer = CartEventsTransformerExtreme(...)
    transformer.process_batch(events)

    # Save
    save_to_parquet(events, chunk_num=chunk_info['chunk_num'])

    return {
        'status': 'success',
        'events_processed': len(events),
        'chunk_num': chunk_info['chunk_num']
    }
```

**Step 5: Monitoring**
```python
# Scheduler monitors Redis queue
while True:
    jobs = get_all_jobs()

    completed = count(jobs, status='finished')
    failed = count(jobs, status='failed')
    running = count(jobs, status='started')

    if completed + failed == total_jobs:
        break  # All done

    sleep(5)
```

**Step 6: Completion**
```
All workers finish
   â””â”€â†’ Scheduler detects completion
       â””â”€â†’ Merges results (if needed)
           â””â”€â†’ Generates final report
               â””â”€â†’ Containers stop
```

---

### **Docker Commands Workflow**

**Single Container:**
```bash
# 1. Build image
docker build -f docker/Dockerfile -t cart-transformer .

# 2. Run transformation
docker run -v $(pwd)/data:/data cart-transformer

# 3. Check output
ls -la data/output/
```

**Worker Pool:**
```bash
# 1. Start all services with 4 workers
docker-compose -f docker/docker-compose.workers.yml up --scale worker=4

# 2. Monitor logs
docker-compose -f docker/docker-compose.workers.yml logs -f scheduler
docker-compose -f docker/docker-compose.workers.yml logs -f worker

# 3. Check Redis queue
docker-compose -f docker/docker-compose.workers.yml exec redis redis-cli LLEN transformation

# 4. Stop all services
docker-compose -f docker/docker-compose.workers.yml down

# 5. Cleanup
docker-compose -f docker/docker-compose.workers.yml down -v
```

---

## ğŸ”— LUá»’NG HOÃ€N CHá»ˆNH END-TO-END

### **Scenario: Process 2 Million Events**

```
PHASE 1: GENERATE DATA (API)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Time: ~5 minutes

1. Start API Server
   $ python main.py
   â””â”€â†’ FastAPI server listening on :8000

2. Generate Events
   $ curl -X POST "http://localhost:8000/cart/generate/events?count=2000000"
   â””â”€â†’ Background task starts
       â”œâ”€â†’ Creates 200K sessions (~30% guest)
       â”œâ”€â†’ Generates 2M events in batches (50K/batch)
       â”œâ”€â†’ Progress: 0% â†’ 100% (real-time tracking)
       â””â”€â†’ Saves: shared_data/private_data/cart_tracking/cart_events.json.gz

3. Check Progress
   $ curl "http://localhost:8000/cart/generate/status"
   {
       "progress_percentage": 75.5,
       "events_generated": 1510000,
       "estimated_time_remaining": "1.2 minutes"
   }

4. Verify Data
   $ curl "http://localhost:8000/cart/statistics"
   {
       "total_events": 2000000,
       "unique_sessions": 200000,
       "unique_customers": 140000  # 30% guest
   }

OUTPUT: cart_events.json.gz (~500MB compressed, ~2GB uncompressed)

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

PHASE 2A: TRANSFORM (Python - Single Process)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Time: ~3-5 minutes
Memory: ~500MB

1. Run Extreme Version
   $ cd transformation
   $ python transform_cart_events_extreme.py \
       --input ../shared_data/private_data/cart_tracking/cart_events.json.gz \
       --output output_extreme \
       --chunk-size 100000

2. Processing Steps (Automated)
   â”œâ”€â†’ Load: Streaming JSON parse (ijson)
   â”œâ”€â†’ Process batches: 100K events at a time
   â”œâ”€â†’ Clean: Remove invalid, fill missing
   â”œâ”€â†’ Deduplicate: Track event_ids across batches
   â”œâ”€â†’ Create Journeys: Session tracking
   â”œâ”€â†’ Save: Partitioned parquet by date
   â””â”€â†’ Generate: session_metrics.parquet

3. Monitor Progress
   [2025-12-16 10:00:00] Processed 100,000 events...
   [2025-12-16 10:00:30] Processed 200,000 events...
   ...
   [2025-12-16 10:04:30] Processed 2,000,000 events
   [2025-12-16 10:04:45] Peak memory usage: ~527 MB

OUTPUT:
â”œâ”€ output_extreme/cart_events_cleaned/ (300 partitions, ~800MB)
â”œâ”€ output_extreme/session_metrics.parquet (~50MB)
â””â”€ output_extreme/transformation_summary.csv

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

PHASE 2B: TRANSFORM (Docker - Worker Pool)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Time: ~1-2 minutes (4 workers)
Memory: ~2.4GB total (600MB x 4)

1. Start Worker Pool
   $ cd transformation/docker
   $ docker-compose -f docker-compose.workers.yml up --scale worker=4

2. Automated Flow
   â”œâ”€â†’ Redis starts
   â”œâ”€â†’ Scheduler analyzes file (2M events)
   â”œâ”€â†’ Splits into 4 chunks (500K each)
   â”œâ”€â†’ Enqueues 4 jobs to Redis
   â”œâ”€â†’ 4 Workers pick up jobs
   â”‚   â”œâ”€ Worker 1: Events 0-500K
   â”‚   â”œâ”€ Worker 2: Events 500K-1M
   â”‚   â”œâ”€ Worker 3: Events 1M-1.5M
   â”‚   â””â”€ Worker 4: Events 1.5M-2M
   â”œâ”€â†’ Each worker processes independently
   â””â”€â†’ All workers save to shared output

3. Monitor Logs
   $ docker-compose -f docker-compose.workers.yml logs -f scheduler

   [scheduler] Splitting 2,000,000 events into 4 chunks
   [scheduler] Enqueued 4 jobs
   [scheduler] Progress: 4/4 completed
   [scheduler] All jobs completed in 1.8 minutes

OUTPUT: Same as Phase 2A but 3x faster

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

PHASE 3: AGGREGATE METRICS
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Time: ~30 seconds

1. Run Aggregation
   $ python transformation/aggregate_metrics.py

2. Processing
   â”œâ”€â†’ Load: cart_events_cleaned/ (all partitions)
   â”œâ”€â†’ Load: session_metrics.parquet
   â”œâ”€â†’ Calculate:
   â”‚   â”œâ”€ Session duration stats
   â”‚   â”œâ”€ Purchase intent (33%)
   â”‚   â”œâ”€ Event distributions
   â”‚   â”œâ”€ Top journey patterns
   â”‚   â””â”€ Time-based metrics
   â””â”€â†’ Generate reports

OUTPUT:
â”œâ”€ aggregation_report.txt (human-readable)
â””â”€ aggregation_metrics.json (machine-readable)

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

PHASE 4: QUERY & ANALYZE (API)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Now you can query transformed data:

1. Session Analysis
   $ curl "http://localhost:8000/cart/events/session/sess_abc123"
   â†’ See complete user journey

2. Customer Behavior
   $ curl "http://localhost:8000/cart/events/customer/445412"
   â†’ All customer interactions

3. Product Performance
   $ curl "http://localhost:8000/cart/events/product/90"
   â†’ Product add/remove history

4. Abandoned Carts
   $ curl "http://localhost:8000/cart/abandoned?hours_threshold=24"
   â†’ Recovery targets

5. Statistics
   $ curl "http://localhost:8000/cart/statistics"
   â†’ Overall metrics
```

---

## ğŸ“Š PERFORMANCE SUMMARY

### **2 Million Events**

| Phase | Method | Time | Memory | Output |
|-------|--------|------|--------|--------|
| **Generate** | API | 5 min | 1GB | 500MB .gz |
| **Transform** | Python Single | 3-5 min | 500MB | 800MB parquet |
| **Transform** | Docker 4 Workers | 1-2 min | 2.4GB | 800MB parquet |
| **Aggregate** | Python | 30s | 1GB | Reports |
| **TOTAL (Single)** | - | **8-10 min** | **500MB** | - |
| **TOTAL (Docker)** | - | **6-8 min** | **2.4GB** | - |

### **Scaling to 10M Events**

| Phase | Method | Time | Memory |
|-------|--------|------|--------|
| **Generate** | API | 25 min | 2GB |
| **Transform** | Python Single | 15-20 min | 600MB |
| **Transform** | Docker 8 Workers | 4-5 min | 4.8GB |
| **Aggregate** | Python | 2 min | 2GB |
| **TOTAL (Single)** | - | **42-47 min** | **600MB** |
| **TOTAL (Docker 8)** | - | **31-32 min** | **4.8GB** |

---

## ğŸ¯ DECISION TREE

```
Báº¡n cÃ³ bao nhiÃªu events?
    â”‚
    â”œâ”€ < 100K events
    â”‚   â””â”€â†’ Python Script (Standard)
    â”‚       $ python transform_cart_events.py
    â”‚
    â”œâ”€ 100K - 500K events
    â”‚   â””â”€â†’ Python Script (Extreme)
    â”‚       $ python transform_cart_events_extreme.py
    â”‚
    â”œâ”€ 500K - 2M events
    â”‚   â”œâ”€â†’ Python (náº¿u khÃ´ng cáº§n nhanh)
    â”‚   â”‚   $ python transform_cart_events_extreme.py
    â”‚   â”‚
    â”‚   â””â”€â†’ Docker Workers (náº¿u cáº§n nhanh)
    â”‚       $ docker-compose -f docker-compose.workers.yml up --scale worker=4
    â”‚
    â””â”€ > 2M events
        â””â”€â†’ Docker Workers Pool (8+ workers)
            $ docker-compose -f docker-compose.workers.yml up --scale worker=8
```

---

## ğŸ“š FILES REFERENCE

```
PROJECT STRUCTURE
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

routers/
â””â”€â”€ cart_tracking_router.py     # API endpoints (generate, query, stats)

transformation/
â”œâ”€â”€ transform_cart_events.py           # Standard (< 100K)
â”œâ”€â”€ transform_cart_events_bigdata.py   # BigData (100K-500K)
â”œâ”€â”€ transform_cart_events_extreme.py   # Extreme (2M+) â­
â”œâ”€â”€ aggregate_metrics.py               # Metrics aggregation
â”œâ”€â”€ run_pipeline.py                    # Main runner
â””â”€â”€ docker/
    â”œâ”€â”€ Dockerfile                     # Single container
    â”œâ”€â”€ Dockerfile.worker              # Worker image
    â”œâ”€â”€ docker-compose.yml             # Single setup
    â”œâ”€â”€ docker-compose.workers.yml     # Worker pool â­
    â”œâ”€â”€ worker.py                      # RQ worker
    â””â”€â”€ scheduler.py                   # Job scheduler

shared_data/
â””â”€â”€ private_data/
    â””â”€â”€ cart_tracking/
        â””â”€â”€ cart_events.json.gz        # Generated events
```

---

## ğŸ”„ QUICK REFERENCE

### **Generate Events**
```bash
curl -X POST "http://localhost:8000/cart/generate/events?count=2000000"
```

### **Check Status**
```bash
curl "http://localhost:8000/cart/generate/status"
```

### **Transform (Python)**
```bash
python transformation/transform_cart_events_extreme.py
```

### **Transform (Docker Single)**
```bash
cd transformation/docker
docker-compose up
```

### **Transform (Docker Workers)**
```bash
cd transformation/docker
docker-compose -f docker-compose.workers.yml up --scale worker=4
```

### **Aggregate**
```bash
python transformation/aggregate_metrics.py
```

### **Query Events**
```bash
curl "http://localhost:8000/cart/events?limit=10&event_type=add_to_cart"
```

### **Statistics**
```bash
curl "http://localhost:8000/cart/statistics"
```

---

**END OF README2.md** ğŸ‰
